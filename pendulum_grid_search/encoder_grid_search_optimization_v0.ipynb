{"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yrAzrGcdzleK","executionInfo":{"status":"ok","timestamp":1651850157932,"user_tz":-120,"elapsed":568,"user":{"displayName":"DamiApache","userId":"14144263532980165864"}},"outputId":"688bb8f3-226f-4fc9-d587-2abbad2028c2"},"execution_count":1,"outputs":[{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["%cd /content/drive/MyDrive/ETH/Master thesis/KalmanNet_VO/"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7TNcWDvWzrsS","executionInfo":{"status":"ok","timestamp":1651850167842,"user_tz":-120,"elapsed":2029,"user":{"displayName":"DamiApache","userId":"14144263532980165864"}},"outputId":"8ac1850d-3719-4c3c-bb91-88e9d100751f"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/ETH/Master thesis/KalmanNet_VO\n"]}]},{"cell_type":"code","source":["!pip install -U ray"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BG2WUQCT0S-F","executionInfo":{"status":"ok","timestamp":1651850183392,"user_tz":-120,"elapsed":13572,"user":{"displayName":"DamiApache","userId":"14144263532980165864"}},"outputId":"70463dde-e1c1-44a4-bd64-0cb5654ea3e3"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting ray\n","  Downloading ray-1.12.0-cp37-cp37m-manylinux2014_x86_64.whl (53.2 MB)\n","\u001b[K     |████████████████████████████████| 53.2 MB 1.2 MB/s \n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from ray) (2.23.0)\n","Collecting grpcio<=1.43.0,>=1.28.1\n","  Downloading grpcio-1.43.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.1 MB)\n","\u001b[K     |████████████████████████████████| 4.1 MB 57.2 MB/s \n","\u001b[?25hRequirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from ray) (1.0.3)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from ray) (3.13)\n","Collecting frozenlist\n","  Downloading frozenlist-1.3.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (144 kB)\n","\u001b[K     |████████████████████████████████| 144 kB 76.8 MB/s \n","\u001b[?25hCollecting aiosignal\n","  Downloading aiosignal-1.2.0-py3-none-any.whl (8.2 kB)\n","Requirement already satisfied: jsonschema in /usr/local/lib/python3.7/dist-packages (from ray) (4.3.3)\n","Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.7/dist-packages (from ray) (7.1.2)\n","Requirement already satisfied: numpy>=1.16 in /usr/local/lib/python3.7/dist-packages (from ray) (1.21.6)\n","Collecting virtualenv\n","  Downloading virtualenv-20.14.1-py2.py3-none-any.whl (8.8 MB)\n","\u001b[K     |████████████████████████████████| 8.8 MB 51.5 MB/s \n","\u001b[?25hRequirement already satisfied: protobuf>=3.15.3 in /usr/local/lib/python3.7/dist-packages (from ray) (3.17.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from ray) (3.6.0)\n","Requirement already satisfied: attrs in /usr/local/lib/python3.7/dist-packages (from ray) (21.4.0)\n","Requirement already satisfied: six>=1.5.2 in /usr/local/lib/python3.7/dist-packages (from grpcio<=1.43.0,>=1.28.1->ray) (1.15.0)\n","Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema->ray) (0.18.1)\n","Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema->ray) (5.7.1)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from jsonschema->ray) (4.2.0)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from jsonschema->ray) (4.11.3)\n","Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from importlib-resources>=1.4.0->jsonschema->ray) (3.8.0)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->ray) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->ray) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->ray) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->ray) (2021.10.8)\n","Collecting distlib<1,>=0.3.1\n","  Downloading distlib-0.3.4-py2.py3-none-any.whl (461 kB)\n","\u001b[K     |████████████████████████████████| 461 kB 84.3 MB/s \n","\u001b[?25hCollecting platformdirs<3,>=2\n","  Downloading platformdirs-2.5.2-py3-none-any.whl (14 kB)\n","Installing collected packages: platformdirs, frozenlist, distlib, virtualenv, grpcio, aiosignal, ray\n","  Attempting uninstall: grpcio\n","    Found existing installation: grpcio 1.44.0\n","    Uninstalling grpcio-1.44.0:\n","      Successfully uninstalled grpcio-1.44.0\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","tensorflow 2.8.0 requires tf-estimator-nightly==2.8.0.dev2021122109, which is not installed.\u001b[0m\n","Successfully installed aiosignal-1.2.0 distlib-0.3.4 frozenlist-1.3.0 grpcio-1.43.0 platformdirs-2.5.2 ray-1.12.0 virtualenv-20.14.1\n"]}]},{"cell_type":"code","execution_count":17,"metadata":{"collapsed":true,"id":"o7bv5eU8zfEc","executionInfo":{"status":"ok","timestamp":1651851154778,"user_tz":-120,"elapsed":588,"user":{"displayName":"DamiApache","userId":"14144263532980165864"}}},"outputs":[],"source":["from functools import partial\n","import numpy as np\n","import os\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.utils.data import random_split\n","import torchvision\n","import torchvision.transforms as transforms\n","import ray\n","from ray import tune\n","from ray.tune import CLIReporter\n","from ray.tune.schedulers import ASHAScheduler\n","from encoder_ae_models import Encoder\n","from torch.utils.data import Dataset, TensorDataset\n","import json\n"]},{"cell_type":"code","execution_count":10,"outputs":[],"source":["import psutil\n","import ray\n","ray._private.utils.get_system_memory = lambda: psutil.virtual_memory().total"],"metadata":{"pycharm":{"name":"#%%\n"},"id":"e58ok5N4zfEf","executionInfo":{"status":"ok","timestamp":1651850709923,"user_tz":-120,"elapsed":2,"user":{"displayName":"DamiApache","userId":"14144263532980165864"}}}},{"cell_type":"code","execution_count":11,"outputs":[],"source":["def flatten_add_noise(dir_input, dir_target, r2, noise_type=\"gaussian\", set_type=\"training_set\", size=1):\n","  input_images = np.load(dir_input)\n","  targets = np.load(dir_target)\n","  if size==1:\n","    input_images = torch.from_numpy(input_images[set_type])\n","    target = torch.from_numpy(targets[set_type])[:, 0:1, :]\n","  else:\n","    till = int(targets[set_type].shape[0] * size)\n","    input_images = torch.from_numpy(input_images[set_type])[:till, ...]\n","    target = torch.from_numpy(targets[set_type])[:till, 0:1, :]\n","\n","  input_images = input_images + torch.normal(mean=0, std=np.sqrt(r2), size=input_images.shape)\n","  normalized_input = torch.clamp(input_images, min=0, max=255) / 255\n","  \n","  return torch.unsqueeze(torch.flatten(normalized_input, 0, 1), 1).float() , torch.flatten(torch.transpose(target, 1, 2), 0, 1).float()\n"],"metadata":{"pycharm":{"name":"#%%\n"},"id":"hHL_s80tzfEi","executionInfo":{"status":"ok","timestamp":1651850711649,"user_tz":-120,"elapsed":2,"user":{"displayName":"DamiApache","userId":"14144263532980165864"}}}},{"cell_type":"code","execution_count":12,"outputs":[],"source":["def train(config, checkpoint_dir=None):\n","    net = Encoder(encoded_dimension=1)\n","\n","    device = \"cpu\"\n","    if torch.cuda.is_available():\n","        device = \"cuda:0\"\n","        if torch.cuda.device_count() > 1:\n","            net = nn.DataParallel(net)\n","    print(f\"Training on {device}.\")\n","    net.to(device)\n","\n","    criterion = nn.MSELoss()\n","    optimizer = optim.Adam(params=net.parameters(), lr=config[\"learning_rate\"])#, weight_decay=config[\"weight_decay\"])\n","\n","\n","    # The `checkpoint_dir` parameter gets passed by Ray Tune when a checkpoint\n","    # should be restored.\n","    if checkpoint_dir:\n","        checkpoint = os.path.join(checkpoint_dir, \"checkpoint\")\n","        model_state, optimizer_state = torch.load(checkpoint)\n","        net.load_state_dict(model_state)\n","        optimizer.load_state_dict(optimizer_state)\n","\n","\n","    inp, target = flatten_add_noise(dir_input=dir_input, dir_target=dir_target, r2=config[\"r2\"], set_type=\"training_set\", size=0.25)\n","    train_subset = TensorDataset(inp, target)\n","    inp_val, target_val = flatten_add_noise(dir_input=dir_input, dir_target=dir_target, r2=config[\"r2\"], set_type=\"validation_set\", size=0.25)\n","    val_subset = TensorDataset(inp_val, target_val)\n","\n","\n","    trainloader = torch.utils.data.DataLoader(\n","        train_subset,\n","        batch_size=int(config[\"batch_size\"]),\n","        shuffle=False,\n","        num_workers=num_work)\n","    valloader = torch.utils.data.DataLoader(\n","        val_subset,\n","        batch_size=int(config[\"batch_size\"]),\n","        shuffle=False,\n","        num_workers=num_work)\n","\n","    for epoch in range(EPOCHS):  # loop over the dataset multiple times\n","        running_loss = 0.0\n","        epoch_steps = 0\n","        net.train()\n","        for i, data in enumerate(trainloader, 0):\n","            # get the inputs; data is a list of [inputs, labels]\n","            inputs, labels = data\n","            inputs, labels = inputs.to(device), labels.to(device)\n","\n","            # zero the parameter gradients\n","            optimizer.zero_grad()\n","\n","            # forward + backward + optimize\n","            outputs = net(inputs)\n","            loss = criterion(outputs, labels)\n","            loss.backward()\n","            optimizer.step()\n","\n","            # print statistics\n","            running_loss += loss.item()\n","            epoch_steps += 1\n","\n","        print(f\"Training loss at epoch {epoch}: {running_loss/epoch_steps}\")\n","        # Validation loss\n","        val_loss = 0.0\n","        val_steps = 0\n","        net.eval()\n","        for i, data in enumerate(valloader, 0):\n","            with torch.no_grad():\n","                inputs, labels = data\n","                inputs, labels = inputs.to(device), labels.to(device)\n","\n","                outputs = net(inputs)\n","                loss = criterion(outputs, labels)\n","                val_loss += loss.cpu().numpy()\n","                val_steps += 1\n","\n","        # Here we save a checkpoint. It is automatically registered with\n","        # Ray Tune and will potentially be passed as the `checkpoint_dir`\n","        # parameter in future iterations.\n","        \"\"\"\n","        with tune.checkpoint_dir(step=epoch) as checkpoint_dir:\n","            path = os.path.join(checkpoint_dir, \"checkpoint\")\n","            torch.save(\n","                (net.state_dict(), optimizer.state_dict()), path)\n","        \"\"\"\n","        tune.report(loss=(val_loss / val_steps))\n","    print(\"Finished Training\")"],"metadata":{"pycharm":{"name":"#%%\n"},"id":"fmmNCi_izfEi","executionInfo":{"status":"ok","timestamp":1651850714801,"user_tz":-120,"elapsed":662,"user":{"displayName":"DamiApache","userId":"14144263532980165864"}}}},{"cell_type":"code","execution_count":26,"outputs":[],"source":["def main(config, num_samples=50, max_num_epochs=100, gpus_per_trial=1/16):\n","    config = config\n","    \n","    scheduler = ASHAScheduler(\n","        max_t=max_num_epochs,\n","        grace_period=1,\n","        reduction_factor=2)\n","    result = tune.run(\n","        tune.with_parameters(train),\n","        resources_per_trial={\"cpu\": 0.5, \"gpu\": gpus_per_trial},\n","        config=config,\n","        metric=\"loss\",\n","        mode=\"min\",\n","        num_samples=num_samples,\n","        scheduler=scheduler\n","    )\n","\n","    best_trial = result.get_best_trial(\"loss\", \"min\", \"last\")\n","    print(\"Best trial config: {}\".format(best_trial.config))\n","    #print(\"Best trial final validation loss: {}\".format(\n","     #   best_trial.last_result[\"loss\"]))\n","  \n","    f  = open(f\"/content/drive/MyDrive/ETH/Master thesis/KalmanNet_VO/pendulum_grid_search/optimal_hyperparameters/optimal_hyperparameters_q2_{q2}_v_{v}.txt\", \"w+\")\n","    f.write(json.dumps(best_trial.config))\n","    f.close()\n","    \n","\n"],"metadata":{"pycharm":{"name":"#%%\n"},"id":"KB4Tv36kzfEk","executionInfo":{"status":"ok","timestamp":1651851703469,"user_tz":-120,"elapsed":400,"user":{"displayName":"DamiApache","userId":"14144263532980165864"}}}},{"cell_type":"code","source":[""],"metadata":{"id":"WyQqkNaHAKTG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["vs = [-20] #in [dB]\n","r2s = [10, 2, 1, 0.5, 0.1, 0.01, 0.001, 0.0001]\n","r2s = [10]\n","num_samples = 1\n","max_epochs = 3\n","EPOCHS = max_epochs\n","num_work = 1\n","training_size= 30000\n","validation_size = 10000\n"," \n","for v in vs:\n","  q2s = list(map(lambda x: x*(10**(v/10)), r2s))\n","  for r2, q2 in zip(r2s, q2s):\n","    dir_input = f\"/content/drive/MyDrive/ETH/Master thesis/KalmanNet_VO/Datasets/Pendulum/images_clean/pendulum_images_clean_q2_{q2:.0e}_v_{v}.npz\"\n","    dir_target = f\"/content/drive/MyDrive/ETH/Master thesis/KalmanNet_VO/Datasets/Pendulum/decimated_clean_data/pendulum_decimated_q2_{q2:.0e}_v_{v}.npz\"\n","    config = {\n","        \"learning_rate\": tune.choice([1e-7, 1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1]),\n","        \"batch_size\": tune.choice([128, 256, 512]),\n","        \"q2\": q2, \"r2\": r2, \"v\": v\n","        }\n","    main(config=config, num_samples=num_samples, max_num_epochs=max_epochs, gpus_per_trial=0.3)"],"metadata":{"id":"QY83Lau_8ZJN","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1651851754673,"user_tz":-120,"elapsed":50444,"user":{"displayName":"DamiApache","userId":"14144263532980165864"}},"outputId":"928ed6de-9cd8-47d1-c417-1dc55048942c"},"execution_count":27,"outputs":[{"output_type":"stream","name":"stderr","text":["2022-05-06 15:41:43,236\tWARNING callback.py:126 -- The TensorboardX logger cannot be instantiated because either TensorboardX or one of it's dependencies is not installed. Please make sure you have the latest version of TensorboardX installed: `pip install -U tensorboardx`\n","2022-05-06 15:41:43,346\tINFO trial_runner.py:803 -- starting train_076f3_00000\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["== Status ==<br>Current time: 2022-05-06 15:41:45 (running for 00:00:02.08)<br>Memory usage on this node: 2.0/25.5 GiB<br>Using AsyncHyperBand: num_stopped=0\n","Bracket: Iter 2.000: None | Iter 1.000: None<br>Resources requested: 0.5/4 CPUs, 0.3/1 GPUs, 0.0/15.0 GiB heap, 0.0/7.5 GiB objects (0.0/1.0 accelerator_type:T4)<br>Result logdir: /root/ray_results/train_2022-05-06_15-41-43<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n","<thead>\n","<tr><th>Trial name       </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  batch_size</th><th style=\"text-align: right;\">  learning_rate</th></tr>\n","</thead>\n","<tbody>\n","<tr><td>train_076f3_00000</td><td>RUNNING </td><td>172.28.0.2:1827</td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">           0.01</td></tr>\n","</tbody>\n","</table><br><br>"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\u001b[2m\u001b[36m(train pid=1827)\u001b[0m Training on cuda:0.\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["== Status ==<br>Current time: 2022-05-06 15:41:50 (running for 00:00:07.09)<br>Memory usage on this node: 4.3/25.5 GiB<br>Using AsyncHyperBand: num_stopped=0\n","Bracket: Iter 2.000: None | Iter 1.000: None<br>Resources requested: 0.5/4 CPUs, 0.3/1 GPUs, 0.0/15.0 GiB heap, 0.0/7.5 GiB objects (0.0/1.0 accelerator_type:T4)<br>Result logdir: /root/ray_results/train_2022-05-06_15-41-43<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n","<thead>\n","<tr><th>Trial name       </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  batch_size</th><th style=\"text-align: right;\">  learning_rate</th></tr>\n","</thead>\n","<tbody>\n","<tr><td>train_076f3_00000</td><td>RUNNING </td><td>172.28.0.2:1827</td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">           0.01</td></tr>\n","</tbody>\n","</table><br><br>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["== Status ==<br>Current time: 2022-05-06 15:41:55 (running for 00:00:12.09)<br>Memory usage on this node: 3.8/25.5 GiB<br>Using AsyncHyperBand: num_stopped=0\n","Bracket: Iter 2.000: None | Iter 1.000: None<br>Resources requested: 0.5/4 CPUs, 0.3/1 GPUs, 0.0/15.0 GiB heap, 0.0/7.5 GiB objects (0.0/1.0 accelerator_type:T4)<br>Result logdir: /root/ray_results/train_2022-05-06_15-41-43<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n","<thead>\n","<tr><th>Trial name       </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  batch_size</th><th style=\"text-align: right;\">  learning_rate</th></tr>\n","</thead>\n","<tbody>\n","<tr><td>train_076f3_00000</td><td>RUNNING </td><td>172.28.0.2:1827</td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">           0.01</td></tr>\n","</tbody>\n","</table><br><br>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["== Status ==<br>Current time: 2022-05-06 15:42:00 (running for 00:00:17.10)<br>Memory usage on this node: 3.8/25.5 GiB<br>Using AsyncHyperBand: num_stopped=0\n","Bracket: Iter 2.000: None | Iter 1.000: None<br>Resources requested: 0.5/4 CPUs, 0.3/1 GPUs, 0.0/15.0 GiB heap, 0.0/7.5 GiB objects (0.0/1.0 accelerator_type:T4)<br>Result logdir: /root/ray_results/train_2022-05-06_15-41-43<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n","<thead>\n","<tr><th>Trial name       </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  batch_size</th><th style=\"text-align: right;\">  learning_rate</th></tr>\n","</thead>\n","<tbody>\n","<tr><td>train_076f3_00000</td><td>RUNNING </td><td>172.28.0.2:1827</td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">           0.01</td></tr>\n","</tbody>\n","</table><br><br>"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\u001b[2m\u001b[36m(train pid=1827)\u001b[0m Training loss at epoch 0: 0.08419080786947351\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["== Status ==<br>Current time: 2022-05-06 15:42:05 (running for 00:00:22.11)<br>Memory usage on this node: 3.8/25.5 GiB<br>Using AsyncHyperBand: num_stopped=0\n","Bracket: Iter 2.000: None | Iter 1.000: None<br>Resources requested: 0.5/4 CPUs, 0.3/1 GPUs, 0.0/15.0 GiB heap, 0.0/7.5 GiB objects (0.0/1.0 accelerator_type:T4)<br>Result logdir: /root/ray_results/train_2022-05-06_15-41-43<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n","<thead>\n","<tr><th>Trial name       </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  batch_size</th><th style=\"text-align: right;\">  learning_rate</th></tr>\n","</thead>\n","<tbody>\n","<tr><td>train_076f3_00000</td><td>RUNNING </td><td>172.28.0.2:1827</td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">           0.01</td></tr>\n","</tbody>\n","</table><br><br>"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Result for train_076f3_00000:\n","  date: 2022-05-06_15-42-07\n","  done: false\n","  experiment_id: 5edd2bd020c04499b25ebfb716005907\n","  hostname: 321eae11af03\n","  iterations_since_restore: 1\n","  loss: 0.0454114900784754\n","  node_ip: 172.28.0.2\n","  pid: 1827\n","  time_since_restore: 21.803831577301025\n","  time_this_iter_s: 21.803831577301025\n","  time_total_s: 21.803831577301025\n","  timestamp: 1651851727\n","  timesteps_since_restore: 0\n","  training_iteration: 1\n","  trial_id: 076f3_00000\n","  warmup_time: 0.003164052963256836\n","  \n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["== Status ==<br>Current time: 2022-05-06 15:42:12 (running for 00:00:28.89)<br>Memory usage on this node: 3.8/25.5 GiB<br>Using AsyncHyperBand: num_stopped=0\n","Bracket: Iter 2.000: None | Iter 1.000: -0.0454114900784754<br>Resources requested: 0.5/4 CPUs, 0.3/1 GPUs, 0.0/15.0 GiB heap, 0.0/7.5 GiB objects (0.0/1.0 accelerator_type:T4)<br>Current best trial: 076f3_00000 with loss=0.0454114900784754 and parameters={'learning_rate': 0.01, 'batch_size': 128, 'q2': 0.1, 'r2': 10, 'v': -20}<br>Result logdir: /root/ray_results/train_2022-05-06_15-41-43<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n","<thead>\n","<tr><th>Trial name       </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  batch_size</th><th style=\"text-align: right;\">  learning_rate</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     loss</th></tr>\n","</thead>\n","<tbody>\n","<tr><td>train_076f3_00000</td><td>RUNNING </td><td>172.28.0.2:1827</td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         21.8038</td><td style=\"text-align: right;\">0.0454115</td></tr>\n","</tbody>\n","</table><br><br>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["== Status ==<br>Current time: 2022-05-06 15:42:17 (running for 00:00:33.90)<br>Memory usage on this node: 3.8/25.5 GiB<br>Using AsyncHyperBand: num_stopped=0\n","Bracket: Iter 2.000: None | Iter 1.000: -0.0454114900784754<br>Resources requested: 0.5/4 CPUs, 0.3/1 GPUs, 0.0/15.0 GiB heap, 0.0/7.5 GiB objects (0.0/1.0 accelerator_type:T4)<br>Current best trial: 076f3_00000 with loss=0.0454114900784754 and parameters={'learning_rate': 0.01, 'batch_size': 128, 'q2': 0.1, 'r2': 10, 'v': -20}<br>Result logdir: /root/ray_results/train_2022-05-06_15-41-43<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n","<thead>\n","<tr><th>Trial name       </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  batch_size</th><th style=\"text-align: right;\">  learning_rate</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     loss</th></tr>\n","</thead>\n","<tbody>\n","<tr><td>train_076f3_00000</td><td>RUNNING </td><td>172.28.0.2:1827</td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         21.8038</td><td style=\"text-align: right;\">0.0454115</td></tr>\n","</tbody>\n","</table><br><br>"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\u001b[2m\u001b[36m(train pid=1827)\u001b[0m Training loss at epoch 1: 0.0675749280277414\n","Result for train_076f3_00000:\n","  date: 2022-05-06_15-42-20\n","  done: false\n","  experiment_id: 5edd2bd020c04499b25ebfb716005907\n","  hostname: 321eae11af03\n","  iterations_since_restore: 2\n","  loss: 0.023699826242689067\n","  node_ip: 172.28.0.2\n","  pid: 1827\n","  time_since_restore: 34.81466293334961\n","  time_this_iter_s: 13.010831356048584\n","  time_total_s: 34.81466293334961\n","  timestamp: 1651851740\n","  timesteps_since_restore: 0\n","  training_iteration: 2\n","  trial_id: 076f3_00000\n","  warmup_time: 0.003164052963256836\n","  \n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["== Status ==<br>Current time: 2022-05-06 15:42:25 (running for 00:00:41.90)<br>Memory usage on this node: 3.8/25.5 GiB<br>Using AsyncHyperBand: num_stopped=0\n","Bracket: Iter 2.000: -0.023699826242689067 | Iter 1.000: -0.0454114900784754<br>Resources requested: 0.5/4 CPUs, 0.3/1 GPUs, 0.0/15.0 GiB heap, 0.0/7.5 GiB objects (0.0/1.0 accelerator_type:T4)<br>Current best trial: 076f3_00000 with loss=0.023699826242689067 and parameters={'learning_rate': 0.01, 'batch_size': 128, 'q2': 0.1, 'r2': 10, 'v': -20}<br>Result logdir: /root/ray_results/train_2022-05-06_15-41-43<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n","<thead>\n","<tr><th>Trial name       </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  batch_size</th><th style=\"text-align: right;\">  learning_rate</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     loss</th></tr>\n","</thead>\n","<tbody>\n","<tr><td>train_076f3_00000</td><td>RUNNING </td><td>172.28.0.2:1827</td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         34.8147</td><td style=\"text-align: right;\">0.0236998</td></tr>\n","</tbody>\n","</table><br><br>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["== Status ==<br>Current time: 2022-05-06 15:42:30 (running for 00:00:46.91)<br>Memory usage on this node: 3.8/25.5 GiB<br>Using AsyncHyperBand: num_stopped=0\n","Bracket: Iter 2.000: -0.023699826242689067 | Iter 1.000: -0.0454114900784754<br>Resources requested: 0.5/4 CPUs, 0.3/1 GPUs, 0.0/15.0 GiB heap, 0.0/7.5 GiB objects (0.0/1.0 accelerator_type:T4)<br>Current best trial: 076f3_00000 with loss=0.023699826242689067 and parameters={'learning_rate': 0.01, 'batch_size': 128, 'q2': 0.1, 'r2': 10, 'v': -20}<br>Result logdir: /root/ray_results/train_2022-05-06_15-41-43<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n","<thead>\n","<tr><th>Trial name       </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  batch_size</th><th style=\"text-align: right;\">  learning_rate</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     loss</th></tr>\n","</thead>\n","<tbody>\n","<tr><td>train_076f3_00000</td><td>RUNNING </td><td>172.28.0.2:1827</td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         34.8147</td><td style=\"text-align: right;\">0.0236998</td></tr>\n","</tbody>\n","</table><br><br>"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\u001b[2m\u001b[36m(train pid=1827)\u001b[0m Training loss at epoch 2: 0.06350612757263034\n","Result for train_076f3_00000:\n","  date: 2022-05-06_15-42-33\n","  done: true\n","  experiment_id: 5edd2bd020c04499b25ebfb716005907\n","  hostname: 321eae11af03\n","  iterations_since_restore: 3\n","  loss: 0.03642699664478432\n","  node_ip: 172.28.0.2\n","  pid: 1827\n","  time_since_restore: 48.072665214538574\n","  time_this_iter_s: 13.258002281188965\n","  time_total_s: 48.072665214538574\n","  timestamp: 1651851753\n","  timesteps_since_restore: 0\n","  training_iteration: 3\n","  trial_id: 076f3_00000\n","  warmup_time: 0.003164052963256836\n","  \n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["== Status ==<br>Current time: 2022-05-06 15:42:33 (running for 00:00:50.17)<br>Memory usage on this node: 3.7/25.5 GiB<br>Using AsyncHyperBand: num_stopped=1\n","Bracket: Iter 2.000: -0.023699826242689067 | Iter 1.000: -0.0454114900784754<br>Resources requested: 0/4 CPUs, 0/1 GPUs, 0.0/15.0 GiB heap, 0.0/7.5 GiB objects (0.0/1.0 accelerator_type:T4)<br>Current best trial: 076f3_00000 with loss=0.03642699664478432 and parameters={'learning_rate': 0.01, 'batch_size': 128, 'q2': 0.1, 'r2': 10, 'v': -20}<br>Result logdir: /root/ray_results/train_2022-05-06_15-41-43<br>Number of trials: 1/1 (1 TERMINATED)<br><table>\n","<thead>\n","<tr><th>Trial name       </th><th>status    </th><th>loc            </th><th style=\"text-align: right;\">  batch_size</th><th style=\"text-align: right;\">  learning_rate</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    loss</th></tr>\n","</thead>\n","<tbody>\n","<tr><td>train_076f3_00000</td><td>TERMINATED</td><td>172.28.0.2:1827</td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">           0.01</td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">         48.0727</td><td style=\"text-align: right;\">0.036427</td></tr>\n","</tbody>\n","</table><br><br>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["2022-05-06 15:42:33,512\tINFO tune.py:702 -- Total run time: 50.28 seconds (50.16 seconds for the tuning loop).\n"]},{"output_type":"stream","name":"stdout","text":["Best trial config: {'learning_rate': 0.01, 'batch_size': 128, 'q2': 0.1, 'r2': 10, 'v': -20}\n"]}]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":2},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython2","version":"2.7.6"},"colab":{"name":"encoder_grid_search_optimization_v-20.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}